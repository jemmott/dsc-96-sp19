{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>An introduction to Machine Learning with Scikit-Learn</h1>\n",
    "    <br /><br />\n",
    "    Created by Gilles Louppe (<a href=\"https://twitter.com/glouppe\">@glouppe</a>) at University of Li√®ge<br />\n",
    "    Modified by Colin Jemmott at UC San Diego\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "* Scikit-Learn and the scientific ecosystem in Python\n",
    "* Classification\n",
    "* Model evaluation and selection\n",
    "* Transformers, pipelines and feature unions\n",
    "* Beyond building classifiers\n",
    "* Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "## Overview\n",
    "\n",
    "* Machine learning library written in __Python__\n",
    "* __Simple and efficient__, for both experts and non-experts\n",
    "* Classical, __well-established machine learning algorithms__\n",
    "* Shipped with <a href=\"http://scikit-learn.org/dev/documentation.html\">documentation</a> and <a href=\"http://scikit-learn.org/dev/auto_examples/index.html\">examples</a>\n",
    "* __BSD 3 license__\n",
    "\n",
    "## Community driven development\n",
    "\n",
    "- 20~ core developers (mostly researchers)\n",
    "- 500-1000 occasional contributors\n",
    "- __All working publicly together__ on [GitHub](https://github.com/scikit-learn/scikit-learn)\n",
    "- Emphasis on __keeping the project maintainable__\n",
    "    - Style consistency\n",
    "    - Unit-test coverage\n",
    "    - Documentation and examples\n",
    "    - Code review\n",
    "- Mature and stable\n",
    "\n",
    "## Python stack for data analysis\n",
    "\n",
    "- The __open source__ Python ecosystem provides __a standalone, versatile and powerful scientific working environment__, including: [NumPy](http://numpy.org), [SciPy](http://scipy.org), [IPython](http://ipython.org), [Matplotlib](http://matplotlib.org), [Pandas](http://pandas.pydata.org/), _and many others..._\n",
    "- Scikit-Learn builds upon NumPy and SciPy and __complements__ this scientific environment with machine learning algorithms;\n",
    "- By design, Scikit-Learn is __non-intrusive__, easy to use and easy to combine with other libraries;\n",
    "- Core algorithms are implemented in low-level languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Algorithms\n",
    "\n",
    "__Unsupervised learning:__\n",
    "\n",
    "* Clustering (KMeans, Ward, ...)\n",
    "* Matrix decomposition (PCA, ICA, ...)\n",
    "* Density estimation\n",
    "* Outlier detection\n",
    "\n",
    "__Model selection and evaluation:__\n",
    "\n",
    "* Cross-validation\n",
    "* Grid-search\n",
    "* Lots of metrics\n",
    "\n",
    "_... and many more!_ (See our [Reference](http://scikit-learn.org/dev/modules/classes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Classification\n",
    "\n",
    "__Supervised learning:__\n",
    "\n",
    "* Linear models (Ridge, Lasso, Elastic Net, ...)\n",
    "* Support Vector Machines\n",
    "* Tree-based methods (Random Forests, Bagging, GBRT, ...)\n",
    "* Nearest neighbors \n",
    "* Neural networks (basics)\n",
    "* Gaussian Processes\n",
    "* Feature selection\n",
    "\n",
    "## Framework\n",
    "\n",
    "Data comes as a finite learning set ${\\cal L} = (X, y)$ where\n",
    "* Input samples are given as an array $X$ of shape `n_samples` $\\times$ `n_features`, taking their values in ${\\cal X}$;\n",
    "* Output values are given as an array $y$, taking _symbolic_ values in ${\\cal Y}$.\n",
    "\n",
    "The goal of supervised classification is to build an estimator $\\varphi: {\\cal X} \\mapsto {\\cal Y}$ minimizing\n",
    "\n",
    "$$\n",
    "Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$.\n",
    "\n",
    "## Applications\n",
    "\n",
    "- Classifying signal from background events; \n",
    "- Diagnosing disease from symptoms;\n",
    "- Recognising cats in pictures;\n",
    "- Identifying body parts with Kinect cameras;\n",
    "- ...\n",
    "  \n",
    "\n",
    "## Data \n",
    "\n",
    "- Input data = Numpy arrays or Scipy sparse matrices ;\n",
    "- Algorithms are expressed using high-level operations defined on matrices or vectors (similar to MATLAB) ;\n",
    "    - Leverage efficient low-leverage implementations ;\n",
    "    - Keep code short and readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Global imports and settings\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"figure.max_open_warning\"] = -1\n",
    "\n",
    "# Print options\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=1000, centers=20, random_state=123)\n",
    "labels = [\"b\", \"r\"]\n",
    "y = np.take(labels, (y < 10))\n",
    "print(X) \n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# X is a 2 dimensional array, with 1000 rows and 2 columns\n",
    "print(X.shape)\n",
    " \n",
    "# y is a vector of 1000 elements\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Rows and columns can be accessed with lists, slices or masks\n",
    "print(X[[1, 2, 3]])     # rows 1, 2 and 3\n",
    "print(X[:5])            # 5 first rows\n",
    "print(X[500:510, 0])    # values from row 500 to row 510 at column 0\n",
    "print(X[y == \"b\"][:5])  # 5 first rows for which y is \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure()\n",
    "for label in labels:\n",
    "    mask = (y == label)\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=label)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loading external data\n",
    "\n",
    "- Numpy provides some [simple tools](https://docs.scipy.org/doc/numpy/reference/routines.io.html) for loading data from files (CSV, binary, etc);\n",
    "\n",
    "- For structured data, Pandas provides more [advanced tools](http://pandas.pydata.org/pandas-docs/stable/io.html) (CSV, JSON, Excel, HDF5, SQL, etc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A simple and unified API\n",
    "\n",
    "All learning algorithms in scikit-learn share a uniform and limited API consisting of complementary interfaces:\n",
    "\n",
    "- an `estimator` interface for building and fitting models;\n",
    "- a `predictor` interface for making predictions;\n",
    "- a `transformer` interface for converting data.\n",
    "\n",
    "Goal: enforce a simple and consistent API to __make it trivial to swap or plug algorithms__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator to data.\"\"\"\n",
    "        # set state of ``self``\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the nearest neighbor class\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Change this to try \n",
    "                                                    # something else\n",
    "\n",
    "# Set hyper-parameters, for controlling algorithm\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Learn a model from training data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Estimator state is stored in instance attributes\n",
    "clf._tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions  \n",
    "print(clf.predict(X[:5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute (approximate) class probabilities\n",
    "print(clf.predict_proba(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tutorial import plot_surface    \n",
    "plot_surface(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tutorial import plot_histogram    \n",
    "plot_histogram(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classifier zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision trees\n",
    "\n",
    "Idea: greedily build a partition of the input space using cuts orthogonal to feature axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial import plot_clf\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "Idea: Build several decision trees with controlled randomness and average their decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "clf = RandomForestClassifier(n_estimators=500)\n",
    "# from sklearn.ensemble import ExtraTreesClassifier \n",
    "# clf = ExtraTreesClassifier(n_estimators=500)\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "Idea: model the decision boundary as an hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Support vector machines\n",
    "\n",
    "Idea: Find the hyperplane which has the largest distance to the nearest training points of any class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")  # try kernel=\"rbf\" instead\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "Idea: a multi-layer perceptron is a circuit of non-linear combinations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation=\"relu\", learning_rate=\"invscaling\")\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "- Recall that we want to learn an estimator $\\varphi$ minimizing the generalization error $Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}$.\n",
    "\n",
    "- Problem: Since $P_{X,Y}$ is unknown, the generalization error $Err(\\varphi)$ cannot be evaluated.\n",
    "\n",
    "- Solution: Use a proxy to approximate $Err(\\varphi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "print(\"Training error =\", zero_one_loss(y, clf.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test error\n",
    "\n",
    "Issue: the training error is a __biased__ estimate of the generalization error.\n",
    "\n",
    "Solution: Divide ${\\cal L}$ into two disjoint parts called training and test sets (usually using 70% for training and 30% for test).\n",
    "- Use the training set for fitting the model;\n",
    "- Use the test set for evaluation only, thereby yielding an unbiased estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training error =\", zero_one_loss(y_train, clf.predict(X_train)))\n",
    "print(\"Test error =\", zero_one_loss(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Summary: Beware of bias when you estimate model performance:\n",
    "- Training score is often an optimistic estimate of the true performance;\n",
    "- __The same data should not be used both for training and evaluation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: \n",
    "- When ${\\cal L}$ is small, training on 70% of the data may lead to a model that is significantly different from a model that would have been learned on the entire set ${\\cal L}$. \n",
    "- Yet, increasing the size of the training set (resp. decreasing the size of the test set), might lead to an inaccurate estimate of the generalization error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solution: K-Fold cross-validation. \n",
    "- Split ${\\cal L}$ into K small disjoint folds. \n",
    "- Train on K-1 folds, evaluate the test error one the held-out fold.\n",
    "- Repeat for all combinations and average the K estimates of the generalization error.\n",
    "\n",
    "<center>![](img/cross-validation.png)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train, test in KFold(n_splits=5, random_state=42).split(X):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    clf = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
    "    scores.append(zero_one_loss(y_test, clf.predict(X_test)))\n",
    "\n",
    "print(\"CV error = %f +-%f\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shortcut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(KNeighborsClassifier(n_neighbors=5), X, y, \n",
    "                         cv=KFold(n_splits=5, random_state=42), \n",
    "                         scoring=\"accuracy\")\n",
    "print(\"CV error = %f +-%f\" % (1. - np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Default score\n",
    "\n",
    "Estimators come with a built-in default evaluation score\n",
    "* Accuracy for classification \n",
    "* R2 score for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = (y_train == \"r\")\n",
    "y_test = (y_test == \"r\")\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train, y_train) \n",
    "print(\"Default score =\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "Definition: The accuracy is the proportion of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy =\", accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision, recall and F-measure\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "print(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\n",
    "print(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\n",
    "print(\"F =\", fbeta_score(y_test, clf.predict(X_test), beta=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ROC AUC\n",
    "\n",
    "Definition: Area under the curve of the false positive rate (FPR) against the true positive rate (TPR) as the decision threshold of the classifier is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer\n",
    "roc_auc_scorer = get_scorer(\"roc_auc\")\n",
    "print(\"ROC AUC =\", roc_auc_scorer(clf, X_test, y_test))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Definition: number of samples of class $i$ predicted as class $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model selection\n",
    " \n",
    "- Finding good hyper-parameters is crucial to control under- and over-fitting, hence achieving better performance.\n",
    "- The estimated generalization error can be used to select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under- and over-fitting\n",
    "\n",
    "- Under-fitting: the model is too simple and does not capture the true relation between X and Y.\n",
    "- Over-fitting: the model is too specific to the training set and does not generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Evaluate parameter range in CV\n",
    "param_range = range(2, 200)\n",
    "param_name = \"max_leaf_nodes\"\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    DecisionTreeClassifier(), X, y, \n",
    "    param_name=param_name, \n",
    "    param_range=param_range, cv=5, n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot parameter VS estimated error\n",
    "plt.xlabel(param_name)\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlim(min(param_range), max(param_range))\n",
    "plt.plot(param_range, 1. - train_scores_mean, color=\"red\", label=\"Training error\")\n",
    "plt.fill_between(param_range, \n",
    "                 1. - train_scores_mean + train_scores_std,\n",
    "                 1. - train_scores_mean - train_scores_std,\n",
    "                 alpha=0.2, color=\"red\")\n",
    "plt.plot(param_range, 1. - test_scores_mean, color=\"blue\", label=\"CV error\")\n",
    "plt.fill_between(param_range, \n",
    "                 1. - test_scores_mean + test_scores_std,\n",
    "                 1. - test_scores_mean - test_scores_std, \n",
    "                 alpha=0.2, color=\"blue\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Best trade-off\n",
    "print(\"%s = %d, CV error = %f\" % (param_name,\n",
    "                                  param_range[np.argmax(test_scores_mean)],\n",
    "                                   1. - np.max(test_scores_mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question: Where is the model under-fitting and over-fitting?\n",
    "\n",
    "Question: What does it mean if the training error is different from the test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(),\n",
    "                    param_grid={\"n_neighbors\": list(range(1, 100))},\n",
    "                    scoring=\"accuracy\",\n",
    "                    cv=5, n_jobs=-1)\n",
    "grid.fit(X, y)  # Note that GridSearchCV is itself an estimator\n",
    "\n",
    "print(\"Best score = %f, Best parameters = %s\" % (1. - grid.best_score_, \n",
    "                                                 grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question: Should you report the best score as an estimate of the generalization error of the model?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
