{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data from the Internet\n",
    "\n",
    "There is a ton of good data on the internet, but it can be hard to access.  In this lesson we will learn just enough about web scraping to get in trouble.  \n",
    "\n",
    "**Important**: stay out of trouble!\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Don't break anything.  Many rapid requests to smaller sites can overload the host server.\n",
    "2. Use a published API if possible - it is more robust and usually much easier!\n",
    "3. Respect the policy published at `robots.txt` \n",
    "4. Don't spoof your UserAgent (or try to trick the server into thinking you are a person)\n",
    "5. Read the Terms of Service for the site and follow it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests\n",
    "\n",
    "`requests` is a python package that allows you to use Python to interact with the internet!  There are other packages, but I find `requests` to be much easier to use.\n",
    "\n",
    "In fact, to get the UCSD home page is a simple as\n",
    "```\n",
    "import requests\n",
    "text = requests.get(\"https://ucsd.edu\").text\n",
    "```\n",
    "But before we do that, we need to learn just a little bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status Codes\n",
    "\n",
    "When we request data from a website, the server responds with a HTTP status code.  The most common response is `200` which means things went well.  Other times you will get a different status code saying something else happened - you might be familiar with a `404` which means the page wasn't found.\n",
    "\n",
    "This great site lists http status codes: [https://httpstat.us/](https://httpstat.us/).\n",
    "\n",
    "But better yet, it has example sites that return a certain code, so you can test!  So, for example, https://httpstat.us/404 returns a `404`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://httpstat.us/404\")\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the call went ok with `r.ok` which returns a boolean.\n",
    "\n",
    "After you run the code below, read up on each of the status codes at [https://httpstat.us/](https://httpstat.us/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statusCodes = [200, 404, 403, 429]\n",
    "\n",
    "for statusCode in statusCodes:\n",
    "    r = requests.get(\"https://httpstat.us/\" + str(statusCode))\n",
    "    print(str(statusCode) + \" ok: \" + str(r.ok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or raise an exception when there is a not-ok status code\n",
    "\n",
    "r = requests.get(\"https://httpstat.us/404\")\n",
    "r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robots.txt\n",
    "\n",
    "Many sites have a published policy allowing or disallowing automatic access to their site.  It uses a text file `robots.txt` and you can learn more about it [here](https://moz.com/learn/seo/robotstxt).\n",
    "\n",
    "The code below checks if the `robot.txt` file prohibits you from scraping the site.  Remember the best practices above - just because you aren't prohibited by the robots policy doesn't mean you can scrape the site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import urllib.robotparser\n",
    "\n",
    "# This code checks the robots.txt file\n",
    "def canFetch(url):\n",
    "\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(domain + \"/robots.txt\")\n",
    "    try:\n",
    "        rp.read()\n",
    "        canFetchBool = rp.can_fetch(\"*\", url)\n",
    "    except:\n",
    "        canFetchBool = None\n",
    "    \n",
    "    return canFetchBool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://slate.com/bullpen/\"\n",
    "canFetch(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dsc.ucsd.edu/node/10\"\n",
    "canFetch(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the HTML\n",
    "\n",
    "Now we can request a website!  Let's see what is on the UCSD Data Science Events page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dsc.ucsd.edu/node/10\"\n",
    "\n",
    "r = requests.get(url)\n",
    "    \n",
    "urlText = r.text\n",
    "\n",
    "Nchars = 1000\n",
    "print(urlText[:Nchars]) # Print the first 500 characters\n",
    "print(\"\\n\\n... \" + str(len(urlText)-Nchars) + \" additional characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "Wow, that is gross looking!  It is raw HTML, which the browser uses to make the viewable site.  To process it we can use [Beautiful Soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "**Warning** BeautifulSoup has changed quite a bit between versions, so make sure you are looking at documentation for the version you are using (4 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(urlText, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all the links\n",
    "for link in soup.find_all('a'):\n",
    "    if link.has_attr('href'):\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the text\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That text had too much white space.  Let's try\n",
    "for string in soup.stripped_strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "From here you can do a number of different things!\n",
    "\n",
    "* Scrape individual elements from a site ([example](https://codeburst.io/web-scraping-101-with-python-beautiful-soup-bb617be1f486))\n",
    "* Pull text down and use NLP from last week (like sentiment analysis)\n",
    "* Monitor a site daily for changes.\n",
    "* Use the text to create your own search engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
